# -*- coding: utf-8 -*-
"""day02_pandas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_g251w1GeR51MpTMeu5PY-43sAuv0_fZ
"""

from google.colab import drive
drive.mount('/content/drive')

## 데이터 불러오기



"""## 데이터 불러오기"""

DATA_PATH ="/content/drive/MyDrive/산대특/python_class/Lemonade2016.csv"
DATA_PATH

import pandas as pd
lemonade = pd.read_csv(DATA_PATH)
lemonade

"""## 정보확인"""

lemonade.info()

lemonade.head(3)

lemonade.tail(3)

lemonade.describe()

"""- 문자형 데이터의 갯수 파악 시, 유용하게 사용"""

lemonade['Location'].value_counts(normalize=True)

"""## 데이터 핸들링
- 행과 열을 다루는 방법
"""

lemonade.head()

"""- 새로운 컬럼 추가"""

lemonade['Sold']=0
lemonade.head()

lemonade['Sold']=lemonade['Lemon']+lemonade['Orange']
lemonade.head()

lemonade['Sold'] = lemonade['Lemon'] + lemonade['Orange']
lemonade.head(3)

"""## Revenue 컬럼 : 매출계산"""

lemonade['Revenue'] = lemonade['Price'] * lemonade['Sold']
lemonade.head(3)

lemonade_col_drop = lemonade.drop('Sold', axis = 1)
lemonade_col_drop.head(4)



"""## 특정 컬럼 삭제"""

# axis =1, 컬럼을 삭제하겠다!
lemonade_col_drop = lemonade.drop('Sold', axis = 1)
lemonade_col_drop.head(3)

# axis = 0 조건에 맞는 행을 삭제하겠다.
lemonade_row_drop = lemonade.drop(0, axis =0)
lemonade_row_drop.head()

lemonade_row_drop = lemonade.drop(3,axis=0)
lemonade_row_drop.reset_index(drop=True)

"""- 행을 삭제한 후, 인덱스 번호를 초기화
  - 0번째~ 순차적으로 시작하도록 변환
"""

lemonade[0:10:2]

"""## 데이터 추출
- 조건식을 사용해서 데이터를 추출
- 조건식에서 참(True)인 값만 추출
"""

lemonade['Location'] == 'Beach'

lemonade[lemonade['Location'] == 'Beach']

lemonade.head()

"""## iloc와 loc


1.   iloc : integer location 데이터 프레임의 행이나 컬럼의 순서를 나타내는 정수로 특정 값을 추출해오는 방법 - 컴퓨터가 읽기 좋은 방식으로 (숫자로)데이터가 있는 위치(순서)에 접근한다고 생각하면 된다.
2.   loc : 컬럼명을 직접 적거나 특정 조건식을 써줌으로써 사람이 읽기 좋은 방법으로 데이터에 접근하는 방법이다.
- df1.loc(0) : 전체 데이터 프레임에서 인덱스 이름이 0인 행만 추출하라
- df1.iloc(0) : 전체 데이터 프레임에서 0번째 행에 있는 값들만 추출하라.




"""

#iloc : df.iloc[행 인덱스:열인덱스]
lemonade.iloc[0:3,0:2]

lemonade.loc[0:2,['Date', 'Location']]

lemonade.head()

#                    행 추출 조건식               열 추출 조건식
result=lemonade.loc[lemonade['Revenue']>100, ['Date', 'Revenue']].reset_index(drop=True)
result

cols = ['Temperature','Date', 'Revenue']
result=lemonade.loc[lemonade['Revenue']>100,cols].reset_index(drop=True)
result

"""## 정렬
- 데이터 프레임 정렬함
- 참조 : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html

"""

lemonade.sort_values(by=['Revenue'], ascending=False).head()

# 오름차순                                                                          같은 값일 경우 오름차순 정렬
lemonade[['Leaflets', 'Revenue']].sort_values(by=['Leaflets','Revenue'], ascending=[False, True]).head()

# 내림차순
lemonade[['Leaflets', 'Revenue']].sort_values(by=['Leaflets','Revenue'], ascending=False).head()

"""## Group by
- SQL Group by와 기능적으로 동일
- ->Location컬럼만 사용
"""



#                                     T- 행과 열을 바꿔줌 
lemonade.groupby(by='Location').count().T

"""### 집계함수(Aggregation)
- groupby와 집계함수 사용하면 -> 피벗테이블
"""

import numpy as np
lemonade.groupby('Location')[['Revenue','Temperature']].agg([max, min, np.mean])

import numpy as np
lemonade.groupby('Location')['Revenue'].agg([max, min, np.mean])

"""## 샘플 데이터 불러오기"""

import seaborn as sns
import numpy as np
import pandas as pd

sns.get_dataset_names()

"""- iris 데이터셋"""

iris = sns.load_dataset('iris')
iris.head()

iris['sepal_length'].std()

iris.info()  # 데이터셋 정보 검색 해야한다.- 비어있는 정보 없다. 인덱스와 갯수 확인하면 된다.

iris.values

iris.values.shape # 배열객체

iris.shape # 데이터 프레임 객체 형태 확인

result = list(iris.columns)
result

"""- 문제
- species 컬럼이 setosa만 전체 조회
"""

iris.head()

result=iris[iris['species']=='setosa']
result

"""- sepal_width 2보다 작은 데이터만 조회"""

result2=result[result['sepal_width'] <3].reset_index(drop=True)
result2

"""- AND 조건 &
- OR 조건 |
-  result3=iris[(조건식1)|(조건식2)|(조건식3)]
"""

result3 = iris[(iris['species']=='setosa') & (iris['sepal_width']<3)].reset_index(drop=True)
result3

result6 = iris[iris['species'] == 'versicolor']
result6

result7=iris[iris['petal_width']>3].reset_index(drop=True)
result7

"""## pandas 숙제 - 0318
- 사이트 : https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



"""- 참조 : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series

- More cookbook : https://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html#cookbook

## series 란
- 앞서 pandas에는 Series와 DataFrame이라는 두 종류의 자료구조가 있다고 했습니다. pandas의 Series는 1차원 배열과 같은 자료구조입니다. 파이썬 리스트와 튜플도 1차원 배열과 같은 자료구조인데 왜 pandas에서는 Series라는 자료구조를 추가로 만든 것일까요?

사실 pandas의 Series는 어떤 면에서는 파이썬의 리스트와 비슷하고 어떤 면에서는 파이썬의 딕셔너리와 닮은 알쏭달쏭한 자료구조입니다. 지금부터 간단히 코드를 작성해가면서 Series라는 자료구조를 공부해 봅시다.

먼저 Series를 사용하기에 앞서 pandas라는 모듈에서 직접 Series와 DataFrame을 로컬 네임스페이스로 임포트합시다.

- 관련 참조 사이트 : https://wikidocs.net/4364
"""

from pandas import Series, DataFrame

"""- Series : 다양한 자료형을 담을 수 있는 1차원의 어레이 엑셀의 하나의 열과 같다
- 레이블을 가진다. index
-DataFrame : 행열을 갖는 2차원의 자료형이다. 여러 개의 Series가 모이면 DataFrame를 구성한다.
- DataFrame 참조 사이트 :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame
"""

dates=pd.date_range("20130101", periods=6)
dates

df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))
df

s1 = pd.Series([1,3,5, np.nan, 6,8])
s = pd.Series([1,3,5, 4, 6,8])
s
s1

data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],
                dtype=[("a", "i4"), ("b", "i4"), ("c", "i4")])
df3 = pd.DataFrame(data, columns=['c', 'a'])

df3

df2=pd.DataFrame(
    {
        "A":1.0,
        "B":pd.Timestamp("20130102"),
        "C":pd.Series(1, index=list(range(4)), dtype="float32"),
        "D":np.array([3]*4, dtype="int32"),
        "E":pd.Categorical(["test","train","test","train"]),
        "F":"foo",
    }
)

df2

df2.dtypes

"""- 역자 주 : 아래 제시된 코드의 경우, IPython이 아닌 환경 (Google Colaboratory, Jupyter 등)에서는 사용이 불가능한 코드인 점에 주의하세요.
- Ipython은 Jupyter의 일부이다

-
df2.A                  df2.bool
df2.abs                df2.boxplot
df2.add                df2.C
df2.add_prefix         df2.clip
df2.add_suffix         df2.clip_lower
df2.align              df2.clip_upper
df2.all                df2.columns
df2.any                df2.combine
df2.append             df2.combine_first
df2.apply              df2.compound
df2.applymap           df2.consolidate
df2.D
"""

df2.<TAB> # noqa :E225, E999

"""## Viewing data"""

df.head()

df.head(-3)

df.head(3)

df.tail()

df.index

df.to_numpy()

"""- For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive:
- to_numpy 참조 : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy
- to_numpy() : DataFrame.to_numpy() does not include the index or column labels in the output. 
"""

df2.to_numpy()

"""- describe() shows a quick statistic summary of your data:"""

df.describe()

"""- Transposing your data: 행과 열을 반전"""

df.T

"""- DataFrame.sort_index() sorts by an axis:"""

df.sort_index(axis=1, ascending=False) # DCBA 로 정렬된다.

"""- DataFrame.sort_values() sorts by values: ascending : True : 오름차순
  "       : False : 내림차순

"""

df.sort_values(by=["A", "C"],ascending=[False, True])

"""## Selection
- While standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, DataFrame.at(), DataFrame.iat(), DataFrame.loc() and DataFrame.iloc().
"""

df["A"]

df[0:3]

"""- Selection by label
See more in Selection by Label using DataFrame.loc() or DataFrame.at().
"""

df.loc[dates[0]]  # dates 첫 행

df.loc[:,["A","B"]]

df.loc["20130102":"20130106",["A","B"]]

df.loc["20130102",["A","B"]]

"""- For getting a scalar value:"""

df.loc[dates[0],"A"]

"""- For getting fast access to a scalar (equivalent to the prior method):"""

df.at[dates[0],"A"]

"""## Selection by position
- See more in Selection by Position using DataFrame.iloc() or DataFrame.at().
- 참조 :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html#pandas.DataFrame.iloc
"""

df.iloc[3]

"""- By integer slices, acting similar to NumPy/Python:"""

df.iloc[3:5, 0:2]

"""- For slicing rows explicitly:"""

df.iloc[1:4,:]

"""- For slicing columns explicitly:"""

df.iloc[:,1:3]

"""- For getting a value explicitly:"""

df.iloc[0,1]

"""- For getting fast access to a scalar (equivalent to the prior method):"""

df.iat[1,1]

"""## Boolean indexing
- Using a single column’s values to select data:
"""

df[df["B"]>0] # B column의 데이터가 0보다 큰 데이터만 선택

df[df>0]

"""- Using the isin() method for filtering: DataFrame.isin : Equivalent method on DataFrame."""

df2=df.copy()

df2["E"]=["one","one","two","three","four","three"]
df2

df2[df2["E"].isin(["two","four","one"])]

"""## Setting
- Setting a new column automatically aligns the data by the indexes:
"""

s1=pd.Series([1,2,3,4,5,6], index=pd.date_range("20130102",periods=6))
s1

"""- Setting values by label:"""

df.at[dates[0],"A"]=0

"""- Setting values by position:"""

df.iat[0,1]=0

"""- Setting by assigning with a NumPy array:"""

df.loc[:,"D"]=np.array([5]*len(df))

"""- The result of the prior setting operations:"""

df

"""- A where operation with setting:"""

df2=df.copy()
df2[df2>0]=-df2
df2

"""## Missing data
- pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section.
- Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data:
"""



df1=df.reindex(index=dates[0:4], columns=list(df.columns)+["E"])
df1.loc[dates[0]:dates[1], "E"]=1
df1

"""- DataFrame.dropna() drops any rows that have missing data:"""

df1.fillna(value=5)

"""- isna() gets the boolean mask where values are nan:"""

pd.isna(df1)

"""## Operations
See the Basic section on Binary Ops.

## Stats
- Operations in general exclude missing data.

- Performing a descriptive statistic:
"""

df.mean()

df.mean(1)

"""- Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension:"""

s=pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
s

df.sub(s, axis="index")

"""## Apply
- DataFrame.apply() applies a user defined function to the data:
"""

df.apply(np.cumsum)

df.apply(lambda x: x.max() - x.min())

"""## Histogramming
See more at Histogramming and Discretization.
"""

s=pd.Series(np.random.randint(0,7,size=10))
s

s.value_counts()

## String Methods
Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them). See more at Vectorized String Methods.

s=pd.Series(["A","B","C","Aaba", "Baca", np.nan,"CABA","dog","cat"])
s.str.lower()

"""## Merge
Concat
pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.

See the Merging section.

- Concatenating pandas objects together along an axis with concat():
- pandas.concat(objs, *, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=True)

"""

df=pd.DataFrame(np.random.randn(10,4))
df

pieces=[df[:3],df[3:7],df[7:]]

pd.concat(pieces)

"""## Note

Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.

## Join
- merge() enables SQL style join types along specific columns. See the Database style joining section.
"""

left =pd.DataFrame({"key":["foo","foo"],"level":[1,2]})
right=pd.DataFrame({"key":["foo","foo"],"rval":[4,5]})
left

right

pd.merge(left,right,on="key")

left=pd.DataFrame({"key":["foo","bar"],"lval":[1,2]})

right=pd.DataFrame({"key":["foo","bar"],"rval":[4,5]})
left

right
pd.merge(left,right,on="key")

"""## Grouping
By “group by” we are referring to a process involving one or more of the following steps:
- Splitting the data into groups based on some criteria
- Applying a function to each group independently
- Combining the results into a data structure

-  See the Grouping section.
"""

df=pd.DataFrame(
    {
    "A":["foo","bar","foo","bar","foo","bar","foo","foo"],
    "B":["one","one","two","three","two","two","one","three"],
    "C":np.random.randn(8),
    "D":np.random.randn(8),
}
)
df

"""- Grouping and then applying the sum() function to the resulting groups:"""

df.groupby("A")[["C","D"]].sum()

"""- Grouping by multiple columns forms a hierarchical index, and again we can apply the sum() function:"""

df.groupby(["A","B"]).sum()

"""## Reshaping
See the sections on Hierarchical Indexing and Reshaping.
- Stack
"""

tuples=list(
    zip(
        ["bar","bar","baz","baz","foo","foo","qux","qux"],
        ["one","two","one","two","one","two","one","two"],
    )
)
index=pd.MultiIndex.from_tuples(tuples,names=["first","second"])
df=pd.DataFrame(np.random.randn(8,2), index=index, columns=["A","B"])
df2=df[:4]
df2

"""- The stack() method “compresses” a level in the DataFrame’s columns:"""

stacked=df2.stack()
stacked

"""- With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:

stacked.unstack()
"""

stacked.unstack()

stacked.unstack(1)

stacked.unstack(0)

"""## Pivot tables
See the section on Pivot Tables.
"""

df=pd.DataFrame(
    {
        "A":["one","one","two","three"]*3,
        "B":["A","B","C"]*4,
        "C":["foo","foo","foo","bar","bar","bar"]*2,
        "D":np.random.randn(12),
        "E":np.random.randn(12),
    }
)
df

"""## pivot_table() pivots a DataFrame specifying the values, index and columns"""

pd.pivot_table(df, values="D", index=["A","B"], columns=["C"])

"""## Time series
pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. See the Time Series section.
"""

rng = pd.date_range("1/1/2012", periods=100, freq="S")
ts=pd.Series(np.random.randint(0,500,len(rng)),index=rng)
ts.resample("5min").sum()

rnd=pd.date_range("3/6/2012 00:00", periods=5, freq="D")
ts=pd.Series(np.random.randn(len(rng)),rng)
ts

ts_utc=ts.tz_localize("UTC")
ts_utc

"""- Series.tz_convert() converts a timezones aware time series to another time zone:"""

ts_utc.tz_convert("US/Eastern")

"""- Converting between time span representations:"""

rng=pd.date_range("1/1/2012",periods=6, freq="M")
ts=pd.Series(np.random.randn(len(rng)), index=rng)
ts

ps=ts.to_period()
ps

ps.to_timestamp()

"""## Categoricals
pandas can include categorical data in a DataFrame. For full docs, see the categorical introduction and the API documentation.
"""

df=pd.DataFrame({
    "id":[1,2,3,4,5,6],"raw_grade":["a","b","b","a","a","e"]
})

df["grade"]=df["raw_grade"].astype("category")
df["grade"]

"""- Rename the categories to more meaningful names:"""

new_categories = ["very good", "good", "very bad"]

df["grade"] = df["grade"].cat.rename_categories(new_categories)

df["grade"] = df["grade"].cat.set_categories(
    ["very bad", "bad", "medium", "good", "very good"]
)


df["grade"]

"""- Sorting is per order in the categories, not lexical order:"""

df.sort_values(by="grade")

"""## Plotting
See the Plotting docs.

We use the standard convention for referencing the matplotlib API:
"""

import matplotlib.pyplot as plt
plt.close("all")

"""-The plt.close method is used to close a figure window:"""

ts=pd.Series(np.random.randn(1000),index=pd.date_range("1/1/2000", periods=1000))
ts=ts.cumsum()
ts.plot()

"""- If running under Jupyter Notebook, the plot will appear on plot(). Otherwise use matplotlib.pyplot.show to show it or matplotlib.pyplot.savefig to write it to a file."""

plt.show();

"""- On a DataFrame, the plot() method is a convenience to plot all of the columns with labels:"""

df=pd.DataFrame(
    np.random.randn(1000,4), index=ts.index, columns=["A","B","C","D"]
)
df=df1
plt.figure();
df.plot();
plt.legend(loc='best');

"""## Importing and exporting data
CSV
Writing to a csv file: using DataFrame.to_csv()
"""

df.to_csv("foo.csv")

"""- Reading from a csv file: using read_csv()"""

pd.read_csv("foo.csv")

"""## HDF5
Reading and writing to HDFStores.

Writing to a HDF5 Store using DataFrame.to_hdf():
"""

df.to_hdf("foo.h5","df")

"""## Reading from a HDF5 Store using read_hdf():"""

pd.read_hdf("foo.h5", "df")

"""## Excel
Reading and writing to Excel.

Writing to an excel file using DataFrame.to_excel():
"""

df.to_excel("foo.xlsx", sheet_name="Sheet1")

"""- Reading from an excel file using read_excel():"""

pd.read_excel("foo.xlsx","Sheet1", index_col=None, na_values=["NA"])

"""## Gotchas
If you are attempting to perform a boolean operation on a Series or DataFrame you might see an exception like:
"""

if pd.Series([False, True, False]):
     print("I was true")

"""- See Comparisons and Gotchas for an explanation and what to do."""